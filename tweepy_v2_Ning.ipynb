{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweepy_v2_Ning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1W_2I7U1PxsFd0CFo5ekbDLEtk1AHWNXR",
      "authorship_tag": "ABX9TyMYz2hH/4s0yfXuR33o8gMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gladcolor/tweet_downloading_v2/blob/master/tweepy_v2_Ning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo8n7XPOlUpO"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_QHH-KFlXfQ"
      },
      "source": [
        "The official [tweets searching progrom](https://github.com/twitterdev/search-tweets-python/tree/v2) is quite good, but I have test it yet. You can try it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jJsa6n1JF6O",
        "outputId": "b452305e-8477-4282-b7a3-1d102ead32eb"
      },
      "source": [
        "pip install searchtweets-v2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting searchtweets-v2\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/10/39bc8e59d1dd000fdf393ceb534eb681eef07acf77f8af9b12389fd5c9a5/searchtweets_v2-1.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from searchtweets-v2) (2.8.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from searchtweets-v2) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from searchtweets-v2) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->searchtweets-v2) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->searchtweets-v2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->searchtweets-v2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->searchtweets-v2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->searchtweets-v2) (3.0.4)\n",
            "Installing collected packages: searchtweets-v2\n",
            "Successfully installed searchtweets-v2-1.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o41IC7uPlcpB"
      },
      "source": [
        "# Prerequisite functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjBwjxgMKSip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef836a47-8dd0-4579-dd2d-a2fc71b4bf2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inep7D3VSIJH"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import logging\n",
        "import tweepy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1mGLosOSQFX"
      },
      "source": [
        "def set_logger(log_file_path=\"debug.log\", level=\"INFO\"):\n",
        "# def set_logger(log_file_path=\"debug.log\", level=\"DEBUG\"):\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(level)\n",
        "    scream_handler = logging.StreamHandler()\n",
        "    file_handler = logging.FileHandler(log_file_path)\n",
        "    logger.addHandler(scream_handler)\n",
        "    logger.addHandler(file_handler)\n",
        "    return logger\n",
        "    \n",
        "  \n",
        "\n",
        "try:\n",
        "    # print(len(logger.handlers))\n",
        "    while len(logger.handlers) > 1:\n",
        "        logger.handlers.pop(0)\n",
        "        # print(len(logger.handlers))\n",
        "except:\n",
        "    pass\n",
        "\n",
        "logger = set_logger()\n",
        "\n",
        "def get_api_token(token_path):    \n",
        "    try:\n",
        "        with open(token_path, \"r\") as f:\n",
        "            logger.debug(\"token_path: %s\" % token_path)\n",
        "            lines = f.readlines()\n",
        "            logger.debug(\"lines in the file: %s\" % lines)\n",
        "\n",
        "            lines = [line.split(\": \")[-1][:-1] for line in lines]\n",
        "        return lines\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(\"Error: %s\" % str(e))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt5HcIf3lsnR"
      },
      "source": [
        "# Set tokens\n",
        "\n",
        "Put your Twitter API tokens in the ```tweet_api_keys.txt``` file in the same directory of this notebook in the following format:\n",
        "```\n",
        "Consumer API Key: XXXX\n",
        "Consumer API Secret Key: XXXX\n",
        "Bearer Token: XXXX\n",
        "Access Token: XXXX\n",
        "Access Token Secret: XXXX\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zva6jeV9lpvH"
      },
      "source": [
        "token_path = r'/content/drive/MyDrive/Research/tweets_downloading/tweet_api_keys.txt'\n",
        "\n",
        "tokens = get_api_token(token_path)\n",
        "\n",
        "consumer_key = tokens[0]\n",
        "consumer_secret = tokens[1]\n",
        "bearer_token = tokens[2]\n",
        "access_token = tokens[3]\n",
        "access_token_secret = tokens[4]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqJ0sVrpntZr"
      },
      "source": [
        "# Download tweets\n",
        "\n",
        "Please set ```query```, ```start_time```, ```end_time```, and ```max_results``` (10 - 500).\n",
        "\n",
        "See these pages to building a query: \n",
        "\n",
        "[Building queries for Search Tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#examples)\n",
        "\n",
        "[Search Tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u3s77VanFtc"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "keyword = \"vaccine\"\n",
        "\n",
        "# query = f\"({keyword}) place_country:AU -is:retweet\"\n",
        "# query = f\"({keyword}) place_country:AU\"\n",
        "query = f\"({keyword})\"\n",
        "start_time = \"2021-01-01T00:00:01Z\"\n",
        "end_time = \"2021-06-01T00:00:01Z\"\n",
        "max_results = 500\n",
        "# saved_path = os.path.join(os.getcwd(), \"saved_tweets\")\n",
        "saved_path = os.path.join(\"/content/drive/MyDrive/Research/tweets_downloading/tweets_no_geo_vaccine\")\n",
        "# since_id = \"139819805172285849\"  # cannot used with start/end_time!\n",
        "\n",
        "\n",
        "# borrow from Twitter:\n",
        "# https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Full-Archive-Search/full-archive-search.py\n",
        "\n",
        "\n",
        "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
        "\n",
        "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
        "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
        "\n",
        "# a exmaple query to download tweets in Australia with a keyword of \"vaccine\" since 2020-01-01\n",
        "# https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all\n",
        "\n",
        "\n",
        "def create_headers(bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    return headers\n",
        "\n",
        "\n",
        "def connect_to_endpoint(url, headers, params):\n",
        "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
        "    # print(response.status_code)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    # return response.json()\n",
        "    return response.json()\n",
        "\n",
        "def save_search(json_response, saved_path):\n",
        "    try:\n",
        "        if not os.path.exists(saved_path):\n",
        "            os.mkdir(saved_path)\n",
        "\n",
        "        meta = json_response['meta']\n",
        "        data = json_response['data']\n",
        "        includes = json_response['includes']\n",
        "        basename = f\"{meta['oldest_id']}_{meta['newest_id']}_{meta['result_count']}\"\n",
        "\n",
        "        data_filename = os.path.join(saved_path, basename + \"_data.csv\")\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(data_filename, index=False)\n",
        "        result_count = meta['result_count']\n",
        "        result_count = str(result_count)\n",
        "        logger.info(\"Saved %s tweets in: %s\" % (result_count, data_filename))\n",
        "\n",
        "        for key in includes.keys():\n",
        "            includes_filename = os.path.join(saved_path, basename + f\"_includes_{key}.csv\")\n",
        "            df = pd.DataFrame(includes[key])\n",
        "            df.to_csv(includes_filename, index=False)\n",
        "    except Exception as e:\n",
        "        logger.error(e, exc_info=True)\n",
        "\n",
        "def execute_download(saved_path=os.getcwd()):\n",
        "\n",
        "    next_token = 'start'\n",
        "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
        "    headers = create_headers(bearer_token)\n",
        "    total = 0\n",
        "    query_params = {'query': query, \\\n",
        "                    \"max_results\": str(max_results), \\\n",
        "                    'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id', \\\n",
        "                    'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld', \\\n",
        "                    'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type', \\\n",
        "                    \"user.fields\": 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\\\n",
        "                    \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics\", \\\n",
        "                    \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\", \\\n",
        "                    \"start_time\": start_time, \\\n",
        "                    \"end_time\": end_time, \\\n",
        "                    # \"since_id\":since_id, \\  # cannot used with start/end_time!\n",
        "                    }\n",
        "\n",
        "    while next_token != \"\":\n",
        "        try:\n",
        "            \n",
        "            json_response = connect_to_endpoint(search_url, headers, query_params)\n",
        "            df = pd.DataFrame(json_response['data'])\n",
        "            save_search(json_response, saved_path)\n",
        "            \n",
        "            total += int(json_response['meta']['result_count'])\n",
        "            logger.info(\"Downloaded %s tweets in total.\" % total)\n",
        "\n",
        "\n",
        "            next_token = json_response['meta'].get('next_token', \"\")\n",
        "            if next_token == \"\":\n",
        "                print(\"No next page! Exit.\")\n",
        "                return\n",
        "\n",
        "            query_params.update({\"next_token\": next_token})\n",
        "            \n",
        "            time.sleep(3)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(e, exc_info=True)\n",
        "            time.sleep(3)\n",
        "            continue\n",
        "\n",
        "\n",
        "execute_download(saved_path=saved_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}